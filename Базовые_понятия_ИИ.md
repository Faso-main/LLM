### Основные термины в машинном обучении и нейросетях

- **Векторизация** — это процесс преобразования тех или иных входных данных в векторы.(https://habr.com/ru/articles/778048/)
  
- **Токенизация** — это процесс разделения, по тем или иным параметрам, разбиения входных данных на более мелкие фрагменты, зависимые или независимые между собой. Токенизация также может рассматриваться как процесс преобразования входных данных в формат, который может быть обработан моделью.

- **Лемматизация** — это метод предварительной обработки текста, используемый в моделях обработки естественного языка (NLP) для разбиения слова на его корневое значение с целью выявления сходств

- **Градиент** — вектор частных производных функции потерь по весам нейронной сети. Он указывает на направление наибольшего роста этой функции для всех весов по совокупности.

- **Скрытое состояние** — это представление входного текста на скрытом слое модели.

- **Эпоха** — это полный проход модели через весь обучающий набор данных, то есть через все партии.

- **Потери (loss)** — это функция, которая измеряет расхождение между предсказанными значениями модели и реальными значениями (метками) в обучающем наборе данных.

- **Веса** — это параметры, которые обновляются в процессе обучения, чтобы минимизировать функцию потерь. Чем лучше модель настраивает свои веса, тем точнее она будет предсказывать выходные данные на основе входных данных.

- **NLP** — Natural Language Processing, обработки естественного языка

- **TF-IDF** (Term Frequency-Inverse Document Frequency) — это числовой статистический показатель, который отражает важность слова для документа.

- **Word2Vec**  — это популярная модель обучения вложений слов, предложенная исследователями Google в 2013 году (Томас Миколов). Она позволяет преобразовать слова из корпуса текстов в векторы чисел таким образом, что слова с похожими семантическими значениями имеют близкие векторные представления в многомерном пространстве. Это делает Word2Vec мощным инструментом для задач обработки естественного языка (NLP), таких как анализ тональности, машинный перевод, автоматическое резюмирование и многие другие.
"https://habr.com/ru/articles/801807/"

"""
Две основные архитектуры модели Word2Vec:
CBOW (Continuous Bag of Words): Этот подход предсказывает текущее слово на основе контекста вокруг него. Например, 
для фразы "синее небо над головой", модель CBOW будет пытаться предсказать слово "небо" на основе 
контекстных слов "синее", "над", "головой". CBOW быстро обрабатывает большие объемы данных, но менее эффективен для редких слов.

Skip-Gram: В этом подходе наоборот, используется текущее слово для предсказания слов в его контексте. 
Для того же примера, модель Skip-Gram будет пытаться предсказать слова "синее", "над", "головой" на основе слова "небо". 
Skip-Gram медленнее обрабатывает данные, но лучше работает с редкими словами и менее частыми контекстами."""


### RAG

- **RAG** (Retrieval Augmented Generation) — это метод работы с большими языковыми моделями

- **N-граммы** — разбиение слов в запросе на последовательности символов длиной N