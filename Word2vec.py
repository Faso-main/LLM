from gensim import models
from pprint import pprint as pp


"https://habr.com/ru/articles/801807/"
"""
Две основные архитектуры модели Word2Vec:
CBOW (Continuous Bag of Words): Этот подход предсказывает текущее слово на основе контекста вокруг него. Например, 
для фразы "синее небо над головой", модель CBOW будет пытаться предсказать слово "небо" на основе 
контекстных слов "синее", "над", "головой". CBOW быстро обрабатывает большие объемы данных, но менее эффективен для редких слов.

Skip-Gram: В этом подходе наоборот, используется текущее слово для предсказания слов в его контексте. 
Для того же примера, модель Skip-Gram будет пытаться предсказать слова "синее", "над", "головой" на основе слова "небо". 
Skip-Gram медленнее обрабатывает данные, но лучше работает с редкими словами и менее частыми контекстами."""

sents = [['люблю', 'собак'],
         ['люди', 'нелюди', 'так', 'сказать'],
         ['люди', 'нелюди', 'так', 'сказать'],
         ['человек', 'радуется', 'такому', 'высказыванию'],
         ['лучшее', 'массивное', 'потсороение', 'взято'],
         ['самое', 'высокое', 'здание', 'в', 'мире', 'было', 'спроектировано', 'с', 'моего', 'скипетра']]

#print(sents)

custom_model = models.Word2Vec(sents, min_count=1,vector_size=300,workers=4)

pp(custom_model.wv.key_to_index)  # Check the learned vocabulary

vect=custom_model.wv.most_similar('собак')

pp(vect)

